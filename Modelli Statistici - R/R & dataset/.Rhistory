0, 1, 1, 0, 1,
0, 1, 1, 1, 0,
0, 1, 1, 1, 1,
1, 0, 0, 0, 0,
1, 0, 0, 0, 1,
1, 0, 0, 1, 0,
1, 0, 0, 1, 1,
1, 0, 1, 0, 0,
1, 0, 1, 0, 1,
1, 0, 1, 1, 0,
1, 0, 1, 1, 1,
1, 1, 0, 0, 0,
1, 1, 0, 0, 1,
1, 1, 0, 1, 0,
1, 1, 0, 1, 1,
1, 1, 1, 0, 0,
1, 1, 1, 0, 1,
1, 1, 1, 1, 0,
1, 1, 1, 1, 1),
ncol= 5, byrow=T
)
### frequenze cumulate del numero di risposte dei patterns
culm = c(3, 9, 11, 22, 23, 24, 27, 31, 32, 40, 40, 56, 56, 59, 61, 76, 86, 115, 129, 210, 213, 241, 256, 336, 352, 408, 429, 602, 613, 674, 702, 1000)
# creaiamo il dataset in cui ogni riga sono le risposte di un soggetto
Data = matrix(NA, ncol=5, nrow=1000)
W1 = 1
for(i in 1:length(culm))
{
W2 = culm[i]
if( (W2-W1)>=0)
{
Data[W1:W2,] = rep(response[i,],each = W2-W1+1)
}
W1 = culm[i]+1
}
# lista con tutti gli elementi che servono al modello
dataList = list(
Y   = Data,
n   = 1000,
k   = 5
)
#### Modello Bayesiano
mod1_string <- '
model
{
# Rasch model
for(i in 1:n)
{
for(j in 1:k)
{
logit(p[i,j]) <- theta[i] - alpha[j]
Y[i,j] ~ dbern(p[i,j])
}
}
# Priors
for(i in 1:n)
{
theta[i] ~ dnorm(0, 1/10000)
}
for(j in 1:(k-1))
{
alpha[j] ~ dnorm(0, 1/10000)
}
# necessaria per identificare i parametri
alpha[k] = 0
}
'
# parametri da salvare
SavePar = c("alpha", "theta", "p")
# fittiamo il modello
set.seed(1)
rasch_model = jags(
data 	= dataList,
parameters.to.save 	= SavePar,
model.file 					= textConnection(mod1_string), # il model.file dovrebbe essere un file,
# "textConnection" crea il file e lo carica
n.chains 						= 1,
n.iter 							= 5000,
n.burnin 						= 2000,
n.thin 							= 2,
DIC 								= T
)
set.seed(42) # for reproducibility
library(ggplot2) # plotting
library(dplyr) # dataframe manipulation
library(tibble) # tibble
n <- 50
rnorm_sample <- rnorm(n) # mu = 0, sigma = 1, for instance
iidplt <- rnorm_sample %>%
enframe() %>% # creates tibble with name,value columns
ggplot() +
geom_point(aes(x = name, y = value)) +
labs(title = "Random sample") + # add title and labels to the plot
xlab("i") +
ylab("x_i")
iidplt
##
rm(list =ls())
## librerie
library(contrast)
library(deldir)
library(tidyverse)
library(aod)
# carichiamo i dati
Data = read.csv("guadagno.csv",sep=",")
summary(Data)
head(Data)
## puliamo e sistemiamo il dataset
#Data = Data[,-1]
recast_Data <- Data %>% select(-x)%>%select(-workclass) %>%
mutate(education = factor(ifelse(education == "Preschool" | education == "10th" | education == "11th" | education == "12th" | education == "1st-4th" | education == "5th-6th" | education == "7th-8th" | education == "9th", "dropout", ifelse(education == "HS-grad", "HighGrad", ifelse(education == "Some-college" | education == "Assoc-acdm" | education == "Assoc-voc", "Community", ifelse(education == "Bachelors", "Bachelors", ifelse(education == "Masters" | education == "Prof-school", "Master", "PhD")))))))
recast_data_2 <-  recast_Data %>% mutate(marital.status = factor(ifelse(marital.status == "Never-married" | marital.status == "Married-spouse-absent", "Not_married", ifelse(marital.status == "Married-AF-spouse" | marital.status == "Married-civ-spouse", "Married", ifelse(marital.status == "Separated" | marital.status == "Divorced", "Separated", "Widow")))))
Data = recast_data_2
###  Numeriche
Data$income = as.factor(Data$income)
plot(Data$age ~ Data$income, col=2:3)
plot(Data$hours.per.week ~ Data$income, col=2:3)
plot(Data$educational.num ~ Data$income, col=2:3)
plot(Data$educational.num~Data$education)
# riscaliamo le variabili
Data_rescale <- Data
for(i in 1:ncol(Data_rescale))
{
if(is.numeric(Data_rescale[,i]))
{
Data_rescale[,i] = scale(Data_rescale[,i])
}
}
attributes(as.factor(Data$gender))
table(Data$gender)
table(Data$gender, Data$income)/nrow(Data)
table(Data$gender, Data$income)/matrix(table(Data$income), ncol=2, nrow=2, byrow=T)
table(Data$gender, Data$income)/matrix(table(Data$gender), ncol=2, nrow=2, byrow=F)
attributes(Data$ marital.status)
table(Data$marital.status, Data$income)/matrix(table(Data$income), ncol=2, nrow=4, byrow=T)
table(Data$marital.status, Data$income)/matrix(table(Data$marital.status), ncol=2, nrow=4, byrow=F)
attributes(as.factor(Data$race))
table(Data$race, Data$income)/matrix(table(Data$income), ncol=2, nrow=5, byrow=T)
table(Data$race, Data$income)/matrix(table(Data$race), ncol=2, nrow=5, byrow=F)
attributes(Data$education )
table(Data$education, Data$income)/matrix(table(Data$income), ncol=2, nrow=6, byrow=T)
table(Data$education, Data$income)/matrix(table(Data$education), ncol=2, nrow=6, byrow=F)
?glm
?family
?make.link
utils::str(make.link("logit"))
# se si voglino cambiare i reference dei fattori
# usare relevel
Data$education = relevel(Data$education,ref="PhD")
Mod1 = glm(income ~ age+education+educational.num+marital.status+race+gender+hours.per.week, data=Data, family=binomial(link = "logit"))
summary(Mod1)
XX = model.matrix(Mod1)
colnames(XX)
W = which(rowSums(XX[,-c(1,2,8,17)])==0 )
# Possiamo fare dei contrasti
Cont1 = contrast(Mod1 ,
list(education="PhD" ,marital.status = "Not_married", race="White", gender="Female",age=26, educational.num= 7, hours.per.week=40),
list(education="Bachelors" ,marital.status = "Not_married", race="White", gender="Female",age=26, educational.num= 7, hours.per.week=40)) # type="individual"
print(Cont1, X=T)
print(Cont1)
# e possiamo calcolarli manualmente
BetaPar = matrix(coef(Mod1),ncol=1)
JpowerLess1= summary(Mod1)$cov.unscaled #
VecConf = Cont1$X
Diff = VecConf%*%BetaPar # contrasto (differenza*varianza)
Var  = VecConf%*%JpowerLess1%*%t(VecConf)
zvalue = Diff/(Var)^0.5
zvalue
pnorm(zvalue,0,1,lower.tail=F)*2
ydic = as.numeric(Data$income)-1
pred = rep(mean(ydic), length(ydic))
W1 = which(ydic==1)
W0 = which(ydic==0)
### il modello è binomiale per la y (mi chiedo se è > o < di 50)
# devianza del modello nullo (dal modello saturo)
D0 =  2*(
sum(log(ydic/pred)[W1])+sum(log((1-ydic  )/(1-pred))[W0])
)
D0
gdl0 = length(ydic)-1 ### togliamo l'intercetta
gdl0
## Model DEviance (residuals)
?predict
pred = predict(Mod1,type="response")
W1 = which(ydic==1)
W0 = which(ydic==0)
### devianza del mio modello (dal modello saturo)
D1 = 2*(
sum(log(ydic/pred)[W1])+sum(log((1-ydic  )/(1-pred))[W0])
)
gdl1 = length(ydic)-length(BetaPar)
D1
gdl1
## testiamo se il modello è meglio del nullo
deltaD = D0-D1
# probabilità che la chiquadro con gdl0-gdl1 gradi di libertà
# sia maggiore del valore deltaD
pchisq(deltaD, gdl0-gdl1, lower.tail=F)
## vediamo se il modello è buono come il saturo
pchisq(D1, gdl1, lower.tail=F)
# rappresentazione grafica
xseq = seq(35000,50000, length.out=100)
plot(xseq,dchisq(xseq,gdl1), type="l")
abline(v=D1,col=2)
xseq = seq(0,20000, length.out=100)
plot(xseq,dchisq(xseq,gdl0-gdl1), type="l")
abline(v=D0-D1,col=2)
rm(list=ls())
#### librerie
library(R2jags)
library(sjmisc)
library(rjags)
rm(list=ls())
# setwd("/Users/gianlucamastrantonio/Dropbox (Politecnico di Torino Staff)/Didattica/Modelli Statistici/Codice R/Bike/")
nyc_bikes = read.csv( "nyc_bike_counts.csv")
summary(nyc_bikes)
# facciamo dei plot
plot(nyc_bikes)
plot(nyc_bikes$hightemp,nyc_bikes$lowtemp)
# facciamo dei plot
plot(nyc_bikes)
plot(nyc_bikes$hightemp,nyc_bikes$lowtemp)
# abbiam problemi di collinearità nelle temperature
mod_freq1 = glm(count ~ weekday+hightemp+lowtemp+precip_rain+precip_snow+time, data = nyc_bikes, family = "poisson")
summary(mod_freq1)
nyc_bikes$weekday
#### Modello Bayesiano
mod1_string <- 'model {
for(i in 1:n) {
Y[i]  ~  dpois(lambda[i])
lambda[i] = exp(mu[i])
mu[i] <- 	b[1]+ #(Intercept)
b[2]*X[i,2]+ #weekdayMonday
b[3]*X[i,3]+ #weekdaySaturday
b[4]*X[i,4]+ #weekdaySunday
b[5]*X[i,5]+ #weekdayThursday
b[6]*X[i,6]+ #weekdayTuesday
b[7]*X[i,7]+ #weekdayWednesday
b[8]*X[i,8]+ #hightemp
b[9]*X[i,9]+ #lowtemp
b[10]*X[i,10]+ #precip_rain
b[11]*X[i,11]+ #precip_snow
b[12]*X[i,12]  # time
}
b[1] ~ dnorm(0,1/1000)
b[2] ~ dnorm(0,1/1000)
b[3] ~ dnorm(0,1/1000)
b[4] ~ dnorm(0,1/1000)
b[5] ~ dnorm(0,1/1000)
b[6] ~ dnorm(0,1/1000)
b[7] ~ dnorm(0,1/1000)
b[8] ~ dnorm(0,1/1000)
b[9] ~ dnorm(0,1/1000)
b[10] ~ dnorm(0,1/1000)
b[11] ~ dnorm(0,1/1000)
b[12] ~ dnorm(0,1/1000)
}'
# inizializzazioni
set.seed(1)
inits =  list(
list("b" = runif(p,-1,1)),
list("b" = runif(p,-1,1))
)
# parametri da salvare
SavePar = c("b", "lambda")
# fittiamo il modello
set.seed(1)
fit_glm1 = jags(
data 								= dataList,
inits 							= inits,
parameters.to.save 	= SavePar,
model.file 					= textConnection(mod1_string), # il model.file dovrebbe essere un file,
# "textConnection" crea il file e lo carica
n.chains 						= nchain,
n.iter 							= 10000,
n.burnin 						= 5000,
n.thin 							= 2,
DIC 								= T
)
rm(list=ls())
# setwd("/Users/gianlucamastrantonio/Dropbox (Politecnico di Torino Staff)/Didattica/Modelli Statistici/Codice R/Bike/")
nyc_bikes = read.csv( "nyc_bike_counts.csv")
summary(nyc_bikes)
# facciamo dei plot
plot(nyc_bikes)
plot(nyc_bikes$hightemp,nyc_bikes$lowtemp)
# abbiam problemi di collinearità nelle temperature
mod_freq1 = glm(count ~ weekday+hightemp+lowtemp+precip_rain+precip_snow+time, data = nyc_bikes, family = "poisson")
summary(mod_freq1)
mod_freq2 = glm(count ~ weekday+lowtemp+precip_rain+precip_snow+time, data = nyc_bikes, family = "poisson")
summary(mod_freq2)
mod_freq3 = glm(count ~ weekday+hightemp+precip_rain+precip_snow+time, data = nyc_bikes, family = "poisson")
summary(mod_freq3)
# vediamo quale modello è migliore, secondo l'AIC
AIC(mod_freq1,mod_freq2,mod_freq3)
# e proviamo a fare model selection con la funzione step
mod_step = step(mod_freq1)
summary(mod_step)
# stimiamo il modello con tutte le covariate
# Matrice X
X = model.matrix(~ weekday+hightemp+lowtemp+precip_rain+precip_snow+time ,data=  nyc_bikes)
# Vettore y
Y = nyc_bikes$count[,drop=F]
# indici vari
n = nrow(X)
p = ncol(X)
nchain = 2
# lista con tutti gli elementi che servono al modello
dataList = list(
Y   = Y,
X   = X,
n   = n
)
#### Modello Bayesiano
mod1_string <- 'model {
for(i in 1:n) {
Y[i]  ~  dpois(lambda[i])
lambda[i] = exp(mu[i])
mu[i] <- 	b[1]+ #(Intercept)
b[2]*X[i,2]+ #weekdayMonday
b[3]*X[i,3]+ #weekdaySaturday
b[4]*X[i,4]+ #weekdaySunday
b[5]*X[i,5]+ #weekdayThursday
b[6]*X[i,6]+ #weekdayTuesday
b[7]*X[i,7]+ #weekdayWednesday
b[8]*X[i,8]+ #hightemp
b[9]*X[i,9]+ #lowtemp
b[10]*X[i,10]+ #precip_rain
b[11]*X[i,11]+ #precip_snow
b[12]*X[i,12]  # time
}
b[1] ~ dnorm(0,1/1000)
b[2] ~ dnorm(0,1/1000)
b[3] ~ dnorm(0,1/1000)
b[4] ~ dnorm(0,1/1000)
b[5] ~ dnorm(0,1/1000)
b[6] ~ dnorm(0,1/1000)
b[7] ~ dnorm(0,1/1000)
b[8] ~ dnorm(0,1/1000)
b[9] ~ dnorm(0,1/1000)
b[10] ~ dnorm(0,1/1000)
b[11] ~ dnorm(0,1/1000)
b[12] ~ dnorm(0,1/1000)
}'
# inizializzazioni
set.seed(1)
inits =  list(
list("b" = runif(p,-1,1)),
list("b" = runif(p,-1,1))
)
# parametri da salvare
SavePar = c("b", "lambda")
# fittiamo il modello
set.seed(1)
fit_glm1 = jags(
data 								= dataList,
inits 							= inits,
parameters.to.save 	= SavePar,
model.file 					= textConnection(mod1_string), # il model.file dovrebbe essere un file,
# "textConnection" crea il file e lo carica
n.chains 						= nchain,
n.iter 							= 10000,
n.burnin 						= 5000,
n.thin 							= 2,
DIC 								= T
)
#### #### #### #### #### #### ####
#### stime
#### #### #### #### #### #### ####
beta_post_chain1 = fit_glm1$BUGSoutput$sims.array[-c(1:100),1,1:p]
beta_post_chain2 = fit_glm1$BUGSoutput$sims.array[-c(1:100),2,1:p]
colnames(beta_post_chain1) = colnames(beta_post_chain2) = colnames(X)
# confrontiamo le catene per vedere se il modello è a convergenza
par(mfrow=c(3,2))
for(i in 1:p)
{
plot(beta_post_chain1[,i], type="l", main= colnames(beta_post_chain1)[i])
lines(beta_post_chain2[,i],col=2)
# if(p%%6)
# {
# 	plot.new()
# }
}
# stimiamo le medie e gli intervalli di credibilità
beta_mean = round(colMeans(beta_post_chain1),5)
beta_CI1  = round(apply(beta_post_chain1,2,quantile,prob=0.025),5)
beta_CI2  = round(apply(beta_post_chain1,2,quantile,prob=0.975),5)
Beta_Res = cbind(beta_mean,beta_CI1,beta_CI2)
Beta_Res
### vediamo le distribuzioni di alcuni parametri
plot(density(beta_post_chain1[,"time"]))
plot(density(beta_post_chain1[,"precip_snow"]))
# c'è molta autocorrelazione tra le temperature
plot(beta_post_chain1[,"hightemp"],beta_post_chain1[,"lowtemp"])
acf(beta_post_chain1[,-c(1:7)])
mod1_string_inf <- 'model {
for(i in 1:n) {
Y[i]  ~  dpois(lambda[i])
lambda[i] = exp(mu[i])
mu[i] <- 	b[1]+ #(Intercept)
b[2]*X[i,2]+ #weekdayMonday
b[3]*X[i,3]+ #weekdaySaturday
b[4]*X[i,4]+ #weekdaySunday
b[5]*X[i,5]+ #weekdayThursday
b[6]*X[i,6]+ #weekdayTuesday
b[7]*X[i,7]+ #weekdayWednesday
b[8]*X[i,8]+ #hightemp
b[9]*X[i,9]+ #lowtemp
b[10]*X[i,10]+ #precip_rain
b[11]*X[i,11]+ #precip_snow
b[12]*X[i,12]  # time
}
b[1] ~ dnorm(0,1/1000)
b[2] ~ dnorm(0,10000)
b[3] ~ dnorm(0,10000)
b[4] ~ dnorm(0,10000)
b[5] ~ dnorm(0,10000)
b[6] ~ dnorm(0,10000)
b[7] ~ dnorm(0,10000)
b[8] ~ dnorm(0,10000)
b[9] ~ dnorm(0,10000)
b[10] ~ dnorm(0,10000)
b[11] ~ dnorm(0,10000)
b[12] ~ dnorm(0,10000)
}'
View(beta_post_chain1)
### vediamo le distribuzioni di alcuni parametri
plot(density(beta_post_chain1[,"time"]))
# pattern di possibili risposte
response = matrix(
c(
0, 0, 0, 0, 0,
0, 0, 0, 0, 1,
0, 0, 0, 1, 0,
0, 0, 0, 1, 1,
0, 0, 1, 0, 0,
0, 0, 1, 0, 1,
0, 0, 1, 1, 0,
0, 0, 1, 1, 1,
0, 1, 0, 0, 0,
0, 1, 0, 0, 1,
0, 1, 0, 1, 0,
0, 1, 0, 1, 1,
0, 1, 1, 0, 0,
0, 1, 1, 0, 1,
0, 1, 1, 1, 0,
0, 1, 1, 1, 1,
1, 0, 0, 0, 0,
1, 0, 0, 0, 1,
1, 0, 0, 1, 0,
1, 0, 0, 1, 1,
1, 0, 1, 0, 0,
1, 0, 1, 0, 1,
1, 0, 1, 1, 0,
1, 0, 1, 1, 1,
1, 1, 0, 0, 0,
1, 1, 0, 0, 1,
1, 1, 0, 1, 0,
1, 1, 0, 1, 1,
1, 1, 1, 0, 0,
1, 1, 1, 0, 1,
1, 1, 1, 1, 0,
1, 1, 1, 1, 1),
ncol= 5, byrow=T
)
### frequenze cumulate del numero di risposte dei patterns
culm = c(3, 9, 11, 22, 23, 24, 27, 31, 32, 40, 40, 56, 56, 59, 61, 76, 86, 115, 129, 210, 213, 241, 256, 336, 352, 408, 429, 602, 613, 674, 702, 1000)
# creaiamo il dataset in cui ogni riga sono le risposte di un soggetto
Data = matrix(NA, ncol=5, nrow=1000)
W1 = 1
for(i in 1:length(culm))
{
W2 = culm[i]
if( (W2-W1)>=0)
{
Data[W1:W2,] = rep(response[i,],each = W2-W1+1)
}
W1 = culm[i]+1
}
# lista con tutti gli elementi che servono al modello
dataList = list(
Y   = Data,
n   = 1000,
k   = 5
)
#### Modello Bayesiano
# theta[i] - alpha[j] bravura studente - difficoltà domanda
mod1_string <- '
model
{
# Rasch model
for(i in 1:n)
{
for(j in 1:k)
{
logit(p[i,j]) <- theta[i] - alpha[j]
Y[i,j] ~ dbern(p[i,j])
}
}
# Priors
for(i in 1:n)
{
theta[i] ~ dnorm(0, 1/10000)
}
for(j in 1:(k-1))
{
alpha[j] ~ dnorm(0, 1/10000)
}
# necessaria per identificare i parametri
alpha[k] = 0
}
'
